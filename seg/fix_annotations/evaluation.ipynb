{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate inferrence with ground truth\n",
    "\n",
    "After inferrence by Mask-RCNN or YOLOv7, convert annotations into coco json format with annotations.ipynb\n",
    "\n",
    "Import the inferred annotation into an annotation software (CVAT by default) to review results. Add/amend/delete segmented masks as necessary.\n",
    "\n",
    "Output the reviewed results in COCO json format. The reviewed results can now be used as the \"ground truth\" to compare with the inferred annotation.\n",
    "\n",
    "Utility codes for evaluation obtained from https://github.com/cocodataset/cocoapi/issues/426 and the library pycocotools is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "__all__ = ['COCOEvaluator']\n",
    "\n",
    "class COCOEvaluator(object):\n",
    "\n",
    "    def __init__(self, anno_gt_file, anno_dt_file):\n",
    "        self.coco_gt = COCO(anno_gt_file)\n",
    "        # self.coco_dt = self.coco_gt.loadRes(anno_dt_file)\n",
    "        self.coco_dt = COCO(anno_dt_file)\n",
    "        self._hack_coco_dt()\n",
    "\n",
    "    def _hack_coco_dt(self):\n",
    "        # inferred file from Mask-R-CNN has score. \n",
    "        # YOLOv7 doesn't support exporting the score in annotation files (although the score is included in the prediction tensor det[:,4] in predict.py)\n",
    "        if 'score' in self.coco_dt.dataset['annotations']: pass\n",
    "        else:\n",
    "            for ann in self.coco_dt.dataset['annotations']:\n",
    "                ann['score'] = 1.0\n",
    "        \n",
    "        # the ground truths (after editing in CVAT) doesn't have scores\n",
    "        for anno in self.coco_gt.dataset['annotations']:\n",
    "            anno['score'] = 1.0\n",
    "\n",
    "    def evaluate(self, iou_type='segm'):\n",
    "        coco_eval = COCOeval(self.coco_gt, self.coco_dt, iou_type)\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "        coco_eval.summarize_per_category()\n",
    "        return coco_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities for annotation file corrections\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def findCategory(data):\n",
    "    # find categories\n",
    "    cats = data[\"categories\"]\n",
    "    category = pd.DataFrame(cats)\n",
    "    category = category.drop(['supercategory'], axis=1)\n",
    "    category = category.rename(columns={'id': 'category_id'})\n",
    "    return category\n",
    "\n",
    "def findImages(data):\n",
    "    img = data[\"images\"]\n",
    "    images = pd.DataFrame(img)\n",
    "    \n",
    "    # unwanted columns exist if exported from CVAT. Not if generated by my code\n",
    "    if set(['license','flickr_url','coco_url','date_captured']).issubset(images.columns):\n",
    "        images = images.drop(columns=['license','flickr_url','coco_url','date_captured'])\n",
    "    \n",
    "    return images\n",
    "\n",
    "def findAnnotations(data):\n",
    "    anno = data[\"annotations\"]\n",
    "    df = pd.DataFrame(anno)\n",
    "    return df\n",
    "\n",
    "# convert all np.integer, np.floating and np.ndarray into json recognisable int, float and lists\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images OK!\n"
     ]
    }
   ],
   "source": [
    "# Files to be evaluated\n",
    "gt_file = './input/checked.json'\n",
    "dt_file = './input/combined.json'\n",
    "\n",
    "# \"image_id\" won't match because CVAT (gt_file) outputs all images. My inference (dt_file) only outputs images with positive annotations\n",
    "# Evaluations are based on the number of images reviewed - hence the nos_image of gt_file\n",
    "# Task here: Correct image_id of the DETECTION FILE (dt_file)\n",
    "\n",
    "# Store categories, images and annotations in separate dataframes\n",
    "with open(gt_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    category = findCategory(data)\n",
    "    images = findImages(data)\n",
    "    nos_image = images['id'].max()\n",
    "with open(dt_file, 'r') as file:\n",
    "    data2 = json.load(file)\n",
    "    category2 = findCategory(data2)\n",
    "    images2 = findImages(data2)\n",
    "    nos_image2 = images2['id'].max()\n",
    "    df2 = findAnnotations(data2)\n",
    "    df2 = df2.merge(images2[['id','file_name']], left_on='image_id', right_on='id')\n",
    "    df2 = df2.rename(columns={'id_x': 'id'})\n",
    "    df2 = df2.drop(columns=['iscrowd','attributes','id_y'])\n",
    "\n",
    "# Check categories\n",
    "for i in range(len(category['name'])):\n",
    "    if category['name'][i] != category2['name'][i]:\n",
    "        print('category id: {} , {} in file 1 different from category id: {} , {} in file 2. Please check'.format(category['category_id'][i], category['name'][i], category2['category_id'][i], category2['name'][i]))\n",
    "# clean category for json dump\n",
    "category = category.rename(columns={'category_id': 'id'})\n",
    "category['supercategory'] = \"\"\n",
    "\n",
    "# Check numbers of images - does the detection file contain fewer images than the ground truth file?\n",
    "if nos_image2 > nos_image:\n",
    "    print(\"detection file contains {} images and ground truth only contains {}. Check file\".format(nos_image2, nos_image))\n",
    "else:\n",
    "    print(\"Number of images OK!\")\n",
    "\n",
    "# Change image id in dt_file to the one of gt_file\n",
    "for i in range(len(df2['id'])):\n",
    "    df2.loc[i, 'image_id'] = images.loc[(images['file_name']==df2['file_name'][i]), 'id'].values\n",
    "df2['iscrowd'] = 0\n",
    "df2['attributes'] = [{'occluded':False}] * len(df2['id'])\n",
    "df2 = df2.drop(columns=['file_name'])\n",
    "\n",
    "# JSON with revised image_id exported for evaluation\n",
    "dict_to_json = {\n",
    "    \"categories\": category.to_dict('records'),\n",
    "    \"images\": images.to_dict('records'),\n",
    "    \"annotations\": df2.to_dict('records')\n",
    "    }\n",
    "with open(\"./input/dt_corrected.json\", \"w\") as outfile:\n",
    "    json.dump(dict_to_json, outfile, cls=NpEncoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.09s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=0.71s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.09s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.094\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.098\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.125\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.125\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.140\n"
     ]
    }
   ],
   "source": [
    "gt_file = gt_file # follow from last comment tab\n",
    "dt_file = './input/dt_corrected.json'\n",
    "\n",
    "eval = COCOEvaluator(anno_gt_file=gt_file, anno_dt_file=dt_file)\n",
    "result = eval.evaluate()\n",
    "\n",
    "# Paste results into a DataFrame\n",
    "# Get titles of metrics\n",
    "metrics = list()\n",
    "with open('./original/metrics.csv', 'r', encoding='utf-8-sig') as file:\n",
    "    metrics = file.readline().strip().split(',')\n",
    "# Assemble Dataframe\n",
    "stats = [list(i) for i in zip(*result.category_stats)]\n",
    "assessed = pd.DataFrame(stats, columns=metrics)\n",
    "# Copy categories\n",
    "assessed['category'] = category['name']\n",
    "metrics.insert(0,'category')\n",
    "assessed = assessed.reindex(columns = metrics)\n",
    "# Export Dataframe\n",
    "assessed.to_csv('evaluated.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segment",
   "language": "python",
   "name": "segment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
